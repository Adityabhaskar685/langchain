{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityabhaskar685/langchain/blob/main/fineTuning_flacon7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGp4qZh4UEKD",
        "outputId": "78449099-0375-4e28-fb5b-690405211f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers --q\n",
        "!pip install xformers --q\n",
        "!pip install einops --q\n",
        "!pip install accelerate --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LHe7EVX4YrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26989a7b-9b5b-4b56-cb19-2310929f2953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets --q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --q\n",
        "!pip install trl --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAIeGHixf82s",
        "outputId": "4048140c-00fa-45e8-d52b-494e5aeb878d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# required libraries\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "hxs3TZx8aAJW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define and Parse Argument\n",
        "\n"
      ],
      "metadata": {
        "id": "OMD-KIUdfkLa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ScriptArguments:\n",
        "  \"\"\"\n",
        "  These arguments vary depending on how many GPU you have, what their capacity and features are,\n",
        "  and what size model you want to train.\n",
        "  \"\"\"\n",
        "\n",
        "  local_rank : Optional[int] = field(default = 1, metadata = {'help': \"Used for multi-gpu\"})\n",
        "\n",
        "  per_device_train_batch_size: Optional[int] = field(default = 4)\n",
        "  per_device_eval_batch_size: Optional[int] = field(default = 1)\n",
        "  gradient_accumulation_steps: Optional[int] = field(default = 4)\n",
        "  learning_rate: Optional[float] = field(default = 2e-4)\n",
        "  max_grad_norm: Optional[float] = field(default = 0.3)\n",
        "  weight_decay: Optional[int] = field(default = 0.001)\n",
        "  lora_alpha: Optional[int] = field(default = 16)\n",
        "  lora_dropout: Optional[float] = field(default = 0.1)\n",
        "  lora_r: Optional[int] = field(default = 64)\n",
        "  max_seq_length: Optional[int] = field(default = 512)\n",
        "  model_name : Optional[str] = field(\n",
        "      default = 'tiiuae/falcon-7b',\n",
        "      metadata= {\n",
        "          'help': \"The model that you want to train from the hugging Face hub. E.g gpt2, gpt2-x1, bert ,etc.\"\n",
        "\n",
        "      },\n",
        "  )\n",
        "\n",
        "  dataset_name: Optional[str] = field(\n",
        "      default = \"timdettmers/openassistant-guanaco\",\n",
        "      metadata = {'help' : \"Thre preference dataset to use.\"},\n",
        "  )\n",
        "\n",
        "  use_4bit: Optional[bool] = field(\n",
        "      default = True,\n",
        "      metadata = {\"help\": \"Activate 4bit precision base model loading\"},\n",
        "\n",
        "  )\n",
        "  use_nested_quant: Optional[bool] = field(\n",
        "      default = False,\n",
        "      metadata = {\"help\": \"Activate nested quantization for 4bit base models\"}\n",
        "\n",
        "  )\n",
        "  bnb_4bit_compute_dtype : Optional[str] = field(\n",
        "      default = 'float16',\n",
        "      metadata = {\"help\": 'Compute dtype for 4bit base models'},\n",
        "  )\n",
        "  bnb_4bit_quant_type : Optional[str] = field(\n",
        "      default = 'nf4',\n",
        "      metadata = {\"help\": 'Quantization type fp4 or nf4'},\n",
        "  )\n",
        "  num_train_epochs : Optional[int] = field(\n",
        "      default = 1,\n",
        "      metadata = {\"help\": \"The number of training epochs for the reward model.\"}\n",
        "  )\n",
        "  fp16: Optional[bool] = field(\n",
        "      default = False,\n",
        "      metadata = {'help' : 'Enables fp16 training.'},\n",
        "  )\n",
        "\n",
        "  bf16: Optional[bool] = field(\n",
        "      default = False,\n",
        "      metadata = {\"help\" : \"Enables bf16 training.\"}\n",
        "  )\n",
        "\n",
        "  packing : Optional[bool] = field(\n",
        "      default = False,\n",
        "      metadata = {\"help\": 'Use packing dataset creating.'}\n",
        "  )\n",
        "\n",
        "  gradient_checkpointing: Optional[bool] = field(\n",
        "      default = True,\n",
        "      metadata = {\"help\": \"Enables gradient checkpointing\"}\n",
        "  )\n",
        "  optim : Optional[str] = field(\n",
        "      default = \"paged_adamw_32bit\",\n",
        "      metadata = {'help' : \"The optimizer to use.\"},\n",
        "\n",
        "  )\n",
        "\n",
        "  lr_schedular_type : str = field(\n",
        "      default = \"constant\",\n",
        "      metadata = {'help': 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'},\n",
        "\n",
        "  )\n",
        "\n",
        "  max_step: int = field(\n",
        "      default = 10000,\n",
        "      metadata = {'help': 'How many optimizer update steps to take.'}\n",
        "  )\n",
        "\n",
        "  warmup_ratio: int = field(default = 0.03, metadata = {'help' : 'Fraction of steps to do a warmup for'})\n",
        "  group_by_length: bool = field(\n",
        "      default = True,\n",
        "      metadata = {\n",
        "          'help' : 'Group sequence into batches with same length. Saves memory and speeds up training considerabley.'\n",
        "      }\n",
        "  )\n",
        "  save_steps: int = field(default = 10, metadata = {\"help\": \"Save checkpoint every X updates steps.\"})\n",
        "  logging_steps : int = field(default = 10, metadata = {\"help\": 'Log every X updates steps'})\n",
        "\n",
        "\n",
        "\n",
        "parser = HfArgumentParser(ScriptArguments)\n",
        "script_args = parser.parse_args_into_dataclasses()[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "RsdgeIrjgtvz",
        "outputId": "03bf7565-9cb5-4de0-f531-9afbde11c746"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 104>\u001b[0m:\u001b[94m104\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mhf_argparser.py\u001b[0m:\u001b[94m355\u001b[0m in                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mparse_args_into_dataclasses\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m352 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m (*outputs, remaining_args)                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m353 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m354 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m remaining_args:                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m355 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mSome specified arguments are not used by the HfArgume\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m356 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m357 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m (*outputs,)                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m358 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mValueError: \u001b[0mSome specified arguments are not used by the HfArgumentParser: \u001b[1m[\u001b[0m\u001b[32m'-f'\u001b[0m, \n",
              "\u001b[32m'/root/.local/share/jupyter/runtime/kernel-e3c4d980-e845-4baa-9d8b-4598633ab531.json'\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 104&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">104</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hf_argparser.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">355</span> in                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parse_args_into_dataclasses</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">352 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> (*outputs, remaining_args)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">353 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">354 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> remaining_args:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>355 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Some specified arguments are not used by the HfArgume</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">356 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">357 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> (*outputs,)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">358 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Some specified arguments are not used by the HfArgumentParser: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'-f'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'/root/.local/share/jupyter/runtime/kernel-e3c4d980-e845-4baa-9d8b-4598633ab531.json'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_prepare_model(args):\n",
        "  compute_dtype = getattr(torch.args.bnd_4bit_compute_dtype)\n",
        "\n",
        "  bnd_config = BitsAndBytesConfig(\n",
        "      load_in_4bit = args.use_4bit,\n",
        "      bnb_4bit_quant_type = args.bnb_4bit_quant_type,\n",
        "      bnb_4bit_compute_dtype= compute_dtype,\n",
        "      bnb_4bit_use_double_quant = args.use_nested_quant\n",
        "  )\n",
        "\n",
        "  if compute_dtype == torch.float16 and args.use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "      print(\"=\" * 80)\n",
        "      print(\"Your GPU support bfloat16, you can accelerate training with the argument --bf16\")\n",
        "      print(\"=\" * 80)\n",
        "    device_map = {\"\": 0}\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name,\n",
        "        quantization_config = bnd_config,\n",
        "        device_map = device_map,\n",
        "        trust_remote_code = True\n",
        "    )\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha = script_args.lora.alpha,\n",
        "        lora_dropout = script_args.lora_dropout,\n",
        "        r = script_args.lora_r,\n",
        "        bias = 'none',\n",
        "        task_type = 'CAUSAL_LM',\n",
        "        target_modules = [\n",
        "            'query_key_value',\n",
        "            'dense',\n",
        "            'dense_h_to_4h',\n",
        "            'dense_4h_to_h'\n",
        "        ], # \"word_embedding\", 'lm_head'\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name,\n",
        "                                              trust_remote_code = True,)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, peft_config, tokenizer"
      ],
      "metadata": {
        "id": "4XmARgKaqIb_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_argument = TrainingArguments(\n",
        "    outdir = './results',\n",
        "    per_device_train_batch_size = script_args.per_device_train_batch_size,\n",
        "    gradient_accumulation_steps = script_args.gradient_accumulation_steps,\n",
        "    optim = script_args.optim,\n",
        "    save_steps = script_args.save_steps,\n",
        "    logging_steps = script_args.logging_steps,\n",
        "    learning_rate = script_args.learning_rate,\n",
        "    fp16 = script_args.fp16,\n",
        "    bf16 = script_args.bf16,\n",
        "    max_grad_norm = script_args.max_grad_norm,\n",
        "    max_steps = script_args.max_steps,\n",
        "    warmup_ratio = script_args.warmup_ratio,\n",
        "    group_by_length = script_args.group_by_length,\n",
        "    lr_schedular_type = script_args.lr_scheduler_type,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "cUOlS39Ureom",
        "outputId": "7d208248-9aa0-42aa-9c9f-7bfb45653a63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m3\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'script_args'\u001b[0m is not defined\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'script_args'</span> is not defined\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model,peft_config, tokenizer = create_and_prepare_model(script_args)\n",
        "model.config.use_cache = False\n",
        "dataset = load_dataset(script_args,dataset_name, split = 'train')\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    peft_config = peft_config,\n",
        "    dataset_text_field = 'text',\n",
        "    max_seq_length = script_args.max_seq_length,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_arguments,\n",
        "    packing = script_args.packing\n",
        ")\n",
        "\n",
        "for name, module in trainer.model.named_modules():\n",
        "  if isinstance(module, LoraLayer):\n",
        "    if script_args.bf16:\n",
        "      module = module.to(torch.bfloat16)\n",
        "  if 'norm' in name:\n",
        "    module = module.to(torch.float32)\n",
        "  if \"lm_head\" in name or 'embed_tokens' in name:\n",
        "    if hasattr(module, 'weight'):\n",
        "      if script_args.bf16 and module.weight.dtype == torch.float32:\n",
        "        module = module.to(torch.bfloat32)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR1es3q9vSt2",
        "outputId": "1f48455b-4651-44cc-87fc-48c4d98ca12c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4pTuIP5vTX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKz6+AL1QcL4yd0s0zQspt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}